{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python codes demonstrate how we can derive the measures of a confusion matrix. The comments embedded in the codes give descriptions to guide the rationale of the programming logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.756\n",
      "Misclassification :  0.244\n",
      "Precision :  0.729\n",
      "Sensitivity/Recall 1:  0.821\n",
      "Specificity/Recall 0:  0.689\n",
      "F1-measure :  0.773\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics \n",
    "import numpy as np \n",
    "\n",
    "#specify dataset source\n",
    "df = pd.read_csv('ChurnFinal.csv') \n",
    "\n",
    "# need to convert categorical to numeric for Python ROC and AUC calculations\n",
    "df_inputs = pd.get_dummies(df[['Gender', 'Age', 'PostalCode', 'Cash', 'CreditCard', \n",
    "            'Cheque', 'SinceLastTrx', 'SqrtTotal', 'SqrtMax', 'SqrtMin']]) \n",
    "df_label = df['Churn']\n",
    "\n",
    "# initiate modelling object, and split train and test sets\n",
    "dtree = DecisionTreeClassifier(criterion = 'entropy', splitter=\"best\", max_depth=5, \n",
    "            min_samples_leaf=5, min_samples_split=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_inputs, df_label, \n",
    "            stratify=df_label, test_size=0.3, random_state=1) \n",
    "\n",
    "# train model with decision tree algorithm  \n",
    "dtree.fit(X_train, y_train) \n",
    "\n",
    "# apply the model to predict data\n",
    "y_predict = dtree.predict(X_test)\n",
    "\n",
    "# Using metrics' function parameters to derive performance measures\n",
    "acc = metrics.accuracy_score(y_test, y_predict) \n",
    "sens = metrics.recall_score(y_test, y_predict,average='binary', pos_label='yes') \n",
    "spec = metrics.recall_score(y_test, y_predict,average='binary', pos_label='no') \n",
    "prec = metrics.precision_score(y_test, y_predict,average='binary', pos_label='yes') \n",
    "f1 = metrics.f1_score(y_test, y_predict,average='binary', pos_label='yes') \n",
    "\n",
    "\n",
    "# display all the measures derived\n",
    "print(\"Accuracy : \", round(acc,3)) \n",
    "print(\"Misclassification : \", round(1-acc,3)) \n",
    "print(\"Precision : \", round(prec,3)) \n",
    "print(\"Sensitivity/Recall 1: \", round(sens,3)) \n",
    "print(\"Specificity/Recall 0: \", round(spec,3)) \n",
    "print(\"F1-measure : \", round(f1,3)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above Python codes, we shall observe the following results:\n",
    "\n",
    "Accuracy : 0.756 \n",
    "Misclassification : 0.244 \n",
    "Precision : 0.729 \n",
    "Sensitivity/Recall 1: 0.821 \n",
    "Specificity/Recall 0: 0.689 \n",
    "F1-measure : 0.773 \n",
    "Based on the results, we can interpret that the performance of the model built as follows:\n",
    "\n",
    "Accuracy: Overall, the model correctly predicts 75.6% of the churn labels (i.e.'yes' and 'no'). \n",
    "\n",
    "Misclassification: Overall, the model misclassified 24.4% of the churn labels (i.e., 'yes' and 'no').\n",
    "\n",
    "Sensitivity: Of all the customers who churned (i.e., 'yes'), 82.1% of them were correctly predicted by the model.\n",
    "\n",
    "Specificity: Of all the customers who were not churned (i.e., 'no'), 68.9% of them were correctly predicted by the model.\n",
    "\n",
    "Precision: 72.9% of those predicted as churn (i.e., 'yes') by the model are actually churned.\n",
    "\n",
    "F1 score: The harmonic mean(average) of the precision and recall/sensitivity is 77.3%.\n",
    "\n",
    "\n",
    "NOTE:  For a detailed explanation of the Python API performance metrics() parameters, refer to the official website, https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
