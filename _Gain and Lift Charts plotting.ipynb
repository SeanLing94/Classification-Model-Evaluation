{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Python codes show examples of how to derive cumulative gain and lift charts using the data modeling algorithm of Decision Tree, using the Python functions of plot_cumulative_gain() and plot_lift_curve(), respectively. \n",
    "\n",
    "The comments embedded in the codes give descriptions to guide the rationale of the programming logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split \n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "\n",
    "# specify dataset source, inputs and target\n",
    "df = pd.read_csv('ChurnFinal.csv')\n",
    "df_inputs = pd.get_dummies(df[['Gender', 'Age', 'PostalCode', 'Cash', 'CreditCard', \n",
    "                'Cheque', 'SinceLastTrx', 'SqrtTotal', 'SqrtMax', 'SqrtMin']])\n",
    "df_label = df['Churn']\n",
    "\n",
    "# initiate modelling objects using differnt algorithms\n",
    "clf_tree = DecisionTreeClassifier(criterion = 'entropy', splitter=\"best\", \n",
    "            max_depth=5, min_samples_leaf=5, min_samples_split=0.1, random_state=7) \n",
    "\n",
    "# spliting train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_inputs, df_label, \n",
    "            stratify=df_label, test_size=0.3, random_state=7) \n",
    "\n",
    "# train models\n",
    "clf_tree.fit(X_train, y_train)\n",
    "\n",
    "# apply models for predictions\n",
    "y_predict = clf_tree.predict_proba(X_test)\n",
    "\n",
    "# plot cummulative gain chart\n",
    "skplt.metrics.plot_cumulative_gain(y_test, y_predict)\n",
    "\n",
    "#add legend information\n",
    "plt.xlabel('Percentile of Sample')\n",
    "plt.ylabel('Gain')\n",
    "plt.title('Cummulative Gain Chart')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n",
    "# save to cummulative gain chart to file\n",
    "import os\n",
    "strFile = \"plot_gain.png\"\n",
    "if os.path.isfile(strFile):\n",
    "   os.remove(strFile)   \n",
    "plt.savefig(strFile) \n",
    "plt.clf()  \n",
    "\n",
    "# plot lift chart\n",
    "skplt.metrics.plot_lift_curve(y_test, y_predict)\n",
    "\n",
    "#add legend information\n",
    "plt.xlabel('Percentile of Sample')\n",
    "plt.ylabel('Lift')\n",
    "plt.title('Lift Chart')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# save to Lift chart to file\n",
    "import os\n",
    "strFile = \"plot_lift.png\"\n",
    "if os.path.isfile(strFile):\n",
    "   os.remove(strFile)   \n",
    "plt.savefig(strFile) \n",
    "plt.clf()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above Python codes, we can observe the cumulative gain and lift charts in plot_gain.png and plot_lift.png, respectively as follows:\n",
    "\n",
    "Recall that gain or lift charts evaluate model performance in a portion of the dataset population. They measure the effectiveness of a classification model calculated as the ratio between the results obtained with and without the model. From the Gain chart above, we can observe that the Decision Tree model performs slightly better (for both labels 'yes' and 'no') than without a model. For example, at the sample's 10% and 40% percentiles, the Decision Tree model achieves a better 50% gain (i.e., 0.2 and 0.65 gains with the model compared to 0.1 and 0.4 gains without the model). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the lift chart above, we can observe that the model is not stable before the 20% percentile of the sample. Approximately at 22% percentile of the sample, the Decision Tree model achieved a 1.8 times lift for the label 'no' and 1.75 times lift for the label 'yes'. This implies that merely using the first 22% samples, and there are 1.75 and 1.8 times more likely we can capture the churned and non-churned customers compared to no model (i.e., a random guess).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
